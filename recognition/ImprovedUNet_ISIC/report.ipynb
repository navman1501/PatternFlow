{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import PIL as pil\n",
    "import os\n",
    "import PIL\n",
    "import pathlib\n",
    "import glob\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "#Set to GPU\n",
    "devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(devices[0], True)\n",
    "\n",
    "#Path\n",
    "images_source = pathlib.Path(\"D:\\s4532664\\COMP3710\\ISIC2018_Task1-2_Training_Data\")\n",
    "\n",
    "#Input images\n",
    "input_dir = images_source / \"ISIC2018_Task1-2_Training_Input_x2\\*.jpg\"\n",
    "\n",
    "#Segmentation images (label images)\n",
    "seg_dir = images_source / \"ISIC2018_Task1_Training_GroundTruth_x2\\*.png\"\n",
    "                             \n",
    "#Setup arrays for training, testing and validation datasets\n",
    "X_train = None\n",
    "X_test = None\n",
    "X_valid = None\n",
    "y_train = None\n",
    "y_test = None\n",
    "y_valid = None\n",
    "\n",
    "training = None\n",
    "validation = None\n",
    "testing = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\s4532664\\COMP3710\\ISIC2018_Task1-2_Training_Data\\ISIC2018_Task1-2_Training_Input_x2\\*.jpg\n",
      "D:\\s4532664\\COMP3710\\ISIC2018_Task1-2_Training_Data\\ISIC2018_Task1_Training_GroundTruth_x2\\*.png\n"
     ]
    }
   ],
   "source": [
    "print(input_dir)\n",
    "print(seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following function will convert a passed-in filename to an image.\n",
    "Processing, such as resizing, normalising (0 to 1) and casting (divide by 255) are done to it to be used in the model.\n",
    "\n",
    "Parameters:\n",
    "    input_file: File name of the input image (scan)\n",
    "    seg_file: File name of the segmentation image (label image)\n",
    "\n",
    "Returns:\n",
    "    Tuple containing 2 tensors of the input image and segmentation image, after processing has been completed\n",
    "\"\"\"\n",
    "def convert_file_image(input_file, seg_file):\n",
    "   \n",
    "    input_image = tf.io.read_file(input_file)\n",
    "    input_image = tf.image.decode_jpeg(input_image, channels = 1)\n",
    "    input_image = tf.image.resize(input_image, [256, 256])\n",
    "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "    \n",
    "    seg_image = tf.io.read_file(seg_file)\n",
    "    seg_image = tf.image.decode_png(seg_image, channels = 1)\n",
    "    seg_image = tf.image.resize(seg_image, [256, 256])\n",
    "    seg_image = seg_image == [0, 255]\n",
    "    \n",
    "    return input_image, seg_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following function creates TF datasets for training, testing and validation purposes, based on the data passed in.\n",
    "The data will be split into the following: 3 datasets (Training - 50%, Validation - 25%, Testing - 25%)\n",
    "The data will also be shuffled, before being split to allow for images of varying difficult to be potentially considered.\n",
    "\n",
    "Parameters:\n",
    "    inputdir: Path of directory that has the input images (scan)\n",
    "    segdir: Path of directory that has the segmentation images (label)\n",
    "\n",
    "Returns:\n",
    "    Tuple containing 3 tensors of the training, validaton and testing datasets.\n",
    "    Each of those datasets has both the input image and segmentation images.\n",
    "\"\"\"\n",
    "def load_data(inputdir, segdir):\n",
    "    \n",
    "    xtrain = sorted(glob.glob(str(inputdir)))\n",
    "    ytrain = sorted(glob.glob(str(segdir)))\n",
    "    \n",
    "    #Shuffle the data\n",
    "    xtrain, ytrain = shuffle(xtrain, ytrain)\n",
    "    \n",
    "    #Split the images into 3 datasets (Training - 50%, Validation - 25%, Testing - 25%)\n",
    "    half_length = int(len(xtrain)/2)\n",
    "    quarter_length_ceil = int(tf.math.ceil(len(xtrain)/4))\n",
    "    \n",
    "    global X_test, X_valid, X_train, y_test, y_valid, y_train\n",
    "        \n",
    "    X_test = xtrain[-(quarter_length_ceil-1):]\n",
    "    X_valid = xtrain[half_length:half_length + quarter_length_ceil]\n",
    "    X_train = xtrain[0:half_length]\n",
    "    \n",
    "    y_test = ytrain[-(quarter_length_ceil-1):]\n",
    "    y_valid = ytrain[half_length:half_length + quarter_length_ceil]\n",
    "    y_train = ytrain[0:half_length]\n",
    "    \n",
    "    #Create datasets\n",
    "    traindata = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    validdata = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "    testdata = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
    "    \n",
    "    #Process every filename and convert to image\n",
    "    traindata = traindata.map(convert_file_image)\n",
    "    validdata = validdata.map(convert_file_image)\n",
    "    testdata = testdata.map(convert_file_image)\n",
    "    \n",
    "    return traindata, validdata, testdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the function of loading data into each dataset\n",
    "training, validation, testing = load_data(input_dir, seg_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The following function will determine the Dice coefficient, which is a spatial overlap index.\n",
    "This measures the overlap between 2 samples and will be used as the metric to determine the performance of the model.\n",
    "The link \"https://towardsdatascience.com/metrics-to-evaluate-your-semantic-segmentation-model-6bcb99639aa2\" by Ekin Tu (2019)\n",
    "assisted with the understanding of the Dice coefficient and the code behind it.\n",
    "\n",
    "Parameters:\n",
    "    y_actual: Actual segmentation image (Ground truth)\n",
    "    y_predicted: Predicted segmentation image (label)\n",
    "\n",
    "Returns:\n",
    "    Dice coefficient\n",
    "\"\"\"\n",
    "def dice_coefficient(y_actual, y_predicted):\n",
    "    y_actual = tf.keras.backend.flatten(y_actual)\n",
    "    y_predicted = tf.keras.backend.flatten(y_predicted)\n",
    "    intersection_y = tf.keras.backend.sum(y_actual * y_predicted)\n",
    "    union_y = tf.keras.backend.sum(y_actual) + tf.keras.backend.sum(y_predicted)\n",
    "    return ((2.0*intersection_y + 1e-7) / (union_y + 1e-7))\n",
    "\n",
    "\"\"\"\n",
    "The following function will determine the Dice distance.\n",
    "This value will show how much of the predicted segmentation image fails to match the actual image.\n",
    "\n",
    "Parameters:\n",
    "    y_actual: Actual segmentation image (Ground truth)\n",
    "    y_predicted: Predicted segmentation image (label)\n",
    "\n",
    "Returns:\n",
    "    Dice loss\n",
    "\"\"\"\n",
    "def dice_loss(y_actual, y_predicted):\n",
    "    return 1 - dice_coefficient(y_actual, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Context module for the Improved U-Net model, for the encoder phase.\n",
    "Contains a pre-activation residual block with 2 3x3x3 convolutional layers\n",
    "and a dropout layer in between (refer to the README file for more information).\n",
    "\n",
    "Parameters:\n",
    "    layer: The tensor that is passed in for the context module to be continued on.\n",
    "    filter_size: Numebr of convolution filters\n",
    "\n",
    "Returns:\n",
    "    layer: The tensor, after the context module has been completed on it\n",
    "\"\"\"\n",
    "def context_module_unet(layer, filter_size):\n",
    "    layer = BatchNormalization()(layer)\n",
    "    layer = Conv2D(filter_size, (3, 3), padding = 'same')(layer)\n",
    "    layer = LeakyReLU(alpha = 0.3)(layer)\n",
    "    layer = Dropout(0.3)(layer)\n",
    "    layer = Conv2D(filter_size, (3, 3), padding = 'same')(layer)\n",
    "    layer = LeakyReLU(alpha = 0.3)(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\"\"\"\n",
    "Upsampling module for the Improved U-Net model, for the decoder phase.\n",
    "Contains a up-sampling layer, followed by a 3x3x3 convolution layer halving the number of feature maps\n",
    "(refer to the README file for more information).\n",
    "\n",
    "Parameters:\n",
    "    layer: The tensor that is passed in for the context module to be continued on.\n",
    "    filter_size: Numebr of convolution filters\n",
    "\n",
    "Returns:\n",
    "    layer: The tensor, after the upsampling module has been completed on it\n",
    "\"\"\"\n",
    "def upsample_module_unet(layer, filter_size):\n",
    "    layer = UpSampling2D()(layer)\n",
    "    layer = Conv2D(filter_size, (3, 3), padding = 'same')(layer)\n",
    "    layer = LeakyReLU(alpha = 0.3)(layer)\n",
    "    \n",
    "    return layer\n",
    "    \n",
    "\"\"\"\n",
    "Localisation module for the Improved U-Net model, for the decoder phase.\n",
    "Used to transfer information that have been encoded by lower levels to higher spatial resolutions.\n",
    "Contains a 3x3x3 convolution layer, followed by a 1x1x1 one\n",
    "(refer to the README file for more information).\n",
    "\n",
    "Parameters:\n",
    "    layer: The tensor that is passed in for the context module to be continued on.\n",
    "    filter_size: Numebr of convolution filters\n",
    "\n",
    "Returns:\n",
    "    layer: The tensor, after the upsampling module has been completed on it\n",
    "\"\"\"\n",
    "def localisation_module_unet(layer, filter_size):\n",
    "    layer = Conv2D(filter_size, (3, 3), padding = 'same')(layer)\n",
    "    layer = LeakyReLU(alpha = 0.3)(layer)\n",
    "    layer = Conv2D(filter_size, (1, 1), padding = 'same')(layer)\n",
    "    layer = LeakyReLU(alpha = 0.3)(layer)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\"\"\"\n",
    "Segmentation layer module for the Improved U-Net model, for the decoder phase.\n",
    "Used for deep supervision in the localisation module\n",
    "(refer to the README file for more information).\n",
    "\n",
    "Parameters:\n",
    "    localise_module_a: Second localisation module run in the U-Net\n",
    "    localise_module_b: Third localisation module run in the U-Net\n",
    "    conv_a: Second-last convolutional layer run in the U-Net\n",
    "\n",
    "Returns:\n",
    "    sum_b: An element-wise summation of the combined\n",
    "\"\"\"\n",
    "def segmentation_layer(localise_module_a, localise_module_b, conv_a):\n",
    "    segment_1 = Conv2D(1, (1, 1), padding = 'same')(localise_module_a)\n",
    "    segment_1 = LeakyReLU(alpha = 0.3)(segment_1)\n",
    "\n",
    "    upsample_a = UpSampling2D()(segment_1)\n",
    "    \n",
    "    segment_2 = Conv2D(1, (1, 1), padding = 'same')(localise_module_b)\n",
    "    segment_2 = LeakyReLU(alpha = 0.3)(segment_2)\n",
    "    sum_a = add([upsample_a, segment_2])\n",
    "    \n",
    "    upsample_b = UpSampling2D()(sum_a)\n",
    "    segment_3 = Conv2D(1, (1, 1), padding = 'same')(conv_a)\n",
    "    segment_3 = LeakyReLU(alpha = 0.3)(segment_3)\n",
    "\n",
    "    sum_b = add([upsample_b, segment_3])\n",
    "    \n",
    "    return sum_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unet_model():\n",
    "    inputs = Input((256, 256, 1))\n",
    "    \n",
    "    conv2D_1 = Conv2D(8, (3, 3), padding = 'same')(inputs)\n",
    "    conv2D_1 = LeakyReLU(alpha = 0.3)(conv2D_1)\n",
    "    \n",
    "    cont_1 = context_module_unet(conv2D_1, 8)\n",
    "    sum_1 = add([conv2D_1, cont_1])\n",
    "    \n",
    "    conv2D_2 = Conv2D(16, (3, 3), padding = 'same', strides = 2)(sum_1)\n",
    "    conv2D_2 = LeakyReLU(alpha = 0.3)(conv2D_2)\n",
    "    cont_2 = context_module_unet(conv2D_2, 16)\n",
    "    sum_2 = add([conv2D_2, cont_2])\n",
    "    \n",
    "    conv2D_3 = Conv2D(32, (3, 3), padding = 'same', strides = 2)(sum_2)\n",
    "    conv2D_3 = LeakyReLU(alpha = 0.3)(conv2D_3)\n",
    "    cont_3 = context_module_unet(conv2D_3, 32)\n",
    "    sum_3 = add([conv2D_3, cont_3])\n",
    "    \n",
    "    conv2D_4 = Conv2D(64, (3, 3), padding = 'same', strides = 2)(sum_3)\n",
    "    conv2D_4 = LeakyReLU(alpha = 0.3)(conv2D_4)\n",
    "    cont_4 = context_module_unet(conv2D_4, 64)\n",
    "    sum_4 = add([conv2D_4, cont_4])\n",
    "    \n",
    "    conv2D_5 = Conv2D(128, (3, 3), padding = 'same', strides = 2)(sum_4)\n",
    "    conv2D_5 = LeakyReLU(alpha = 0.3)(conv2D_5)\n",
    "    cont_5 = context_module_unet(conv2D_5, 128)\n",
    "    sum_5 = add([conv2D_5, cont_5])\n",
    "    \n",
    "    upsample_1 = upsample_module_unet(sum_5, 64)\n",
    "    concatenate_1 = concatenate([upsample_1, sum_4])\n",
    "    \n",
    "    localise_1 = localisation_module_unet(concatenate_1, 64)\n",
    "    upsample_2 = upsample_module_unet(localise_1, 32)\n",
    "    concatenate_2 = concatenate([upsample_2, sum_3])\n",
    "    \n",
    "    localise_2 = localisation_module_unet(concatenate_2, 32)\n",
    "    upsample_3 = upsample_module_unet(localise_2, 16)\n",
    "    concatenate_3 = concatenate([upsample_3, sum_2])\n",
    "    \n",
    "    localise_3 = localisation_module_unet(concatenate_3, 16)\n",
    "    upsample_4 = upsample_module_unet(localise_3, 8)\n",
    "    concatenate_4 = concatenate([upsample_4, sum_1])\n",
    "    \n",
    "    conv2D_6 = Conv2D(16, (3, 3), padding = 'same')(concatenate_4)\n",
    "    conv2D_6 = LeakyReLU(alpha = 0.3)(conv2D_6)\n",
    "    \n",
    "    segmentation_1 = segmentation_layer(localise_2, localise_3, conv2D_6)\n",
    "    \n",
    "    conv2D_final = Conv2D(2, (1, 1), activation = 'sigmoid', padding = 'same')(segmentation_1)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=conv2D_final)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=[dice_coefficient])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 130 steps, validate for 65 steps\n",
      "Epoch 1/30\n",
      "130/130 [==============================] - 71s 549ms/step - loss: 0.5391 - dice_coefficient: 0.6755 - val_loss: 0.5686 - val_dice_coefficient: 0.6406\n",
      "Epoch 2/30\n",
      "130/130 [==============================] - 47s 361ms/step - loss: 0.4339 - dice_coefficient: 0.7448 - val_loss: 0.4373 - val_dice_coefficient: 0.7055\n",
      "Epoch 3/30\n",
      "130/130 [==============================] - 57s 437ms/step - loss: 0.3298 - dice_coefficient: 0.8124 - val_loss: 0.3182 - val_dice_coefficient: 0.8175\n",
      "Epoch 4/30\n",
      "130/130 [==============================] - 27s 206ms/step - loss: 0.2697 - dice_coefficient: 0.8456 - val_loss: 0.2714 - val_dice_coefficient: 0.8508\n",
      "Epoch 5/30\n",
      "130/130 [==============================] - 57s 436ms/step - loss: 0.2403 - dice_coefficient: 0.8616 - val_loss: 0.2286 - val_dice_coefficient: 0.8720\n",
      "Epoch 6/30\n",
      "130/130 [==============================] - 77s 590ms/step - loss: 0.2131 - dice_coefficient: 0.8771 - val_loss: 0.2053 - val_dice_coefficient: 0.8843\n",
      "Epoch 7/30\n",
      "130/130 [==============================] - 77s 590ms/step - loss: 0.2021 - dice_coefficient: 0.8835 - val_loss: 0.1940 - val_dice_coefficient: 0.8922\n",
      "Epoch 8/30\n",
      "130/130 [==============================] - 37s 283ms/step - loss: 0.1886 - dice_coefficient: 0.8915 - val_loss: 0.1981 - val_dice_coefficient: 0.8933\n",
      "Epoch 9/30\n",
      "130/130 [==============================] - 77s 591ms/step - loss: 0.1822 - dice_coefficient: 0.8948 - val_loss: 0.1830 - val_dice_coefficient: 0.8903\n",
      "Epoch 10/30\n",
      "130/130 [==============================] - 67s 512ms/step - loss: 0.1721 - dice_coefficient: 0.9006 - val_loss: 0.1915 - val_dice_coefficient: 0.8949\n",
      "Epoch 11/30\n",
      "130/130 [==============================] - 77s 590ms/step - loss: 0.1650 - dice_coefficient: 0.9044 - val_loss: 0.2094 - val_dice_coefficient: 0.8945\n",
      "Epoch 12/30\n",
      "130/130 [==============================] - 57s 436ms/step - loss: 0.1598 - dice_coefficient: 0.9074 - val_loss: 0.2260 - val_dice_coefficient: 0.8987\n",
      "Epoch 13/30\n",
      "130/130 [==============================] - 57s 436ms/step - loss: 0.1574 - dice_coefficient: 0.9086 - val_loss: 0.1960 - val_dice_coefficient: 0.8957\n",
      "Epoch 14/30\n",
      " 43/130 [========>.....................] - ETA: 12s - loss: 0.1565 - dice_coefficient: 0.9094"
     ]
    }
   ],
   "source": [
    "model = unet_model()\n",
    "results = model.fit(x=training.batch(10), epochs = 30, validation_data = validation.batch(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(results.history['dice_coefficient'], label='Training data accuracy')\n",
    "plt.plot(results.history['val_dice_coefficient'], label = 'Test data accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Dice coefficient')\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "test_loss, test_acc = model.evaluate(x=testing.batch(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image, mask in testing.take(20):\n",
    "    pred_mask = model.predict(image[tf.newaxis,...])\n",
    "    pred_mask = tf.argmax(pred_mask[0],axis=-1)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "\n",
    "    \n",
    "    disp_list = [tf.squeeze(image), tf.argmax(mask, axis=-1), pred_mask]\n",
    "\n",
    "    for i in range(3):\n",
    "        plt.subplot(1, 3, i+1)\n",
    "        plt.imshow(disp_list[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
